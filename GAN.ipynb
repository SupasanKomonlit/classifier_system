{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network\n",
    "\n",
    "reference : https://towardsdatascience.com/generate-anime-style-face-using-dcgan-and-explore-its-latent-feature-representation-ae0e905f3974\n",
    "\n",
    "## Header Program\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import library create by mysel\n",
    "from library.directory_handle import DirectoryHandle\n",
    "import library.image_handle as ImageHandle\n",
    "import library.data_handle as DataHandle\n",
    "\n",
    "# Import library for plot image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import library for manage model part Core Layers\n",
    "from keras.layers import Input, Flatten, Dense, Reshape, Lambda, Dropout\n",
    "# Import library for manage model part Convolution Layers\n",
    "from keras.layers import Conv2D, Conv2DTranspose\n",
    "# Import library for manage model part Activation\n",
    "from keras.layers import Activation\n",
    "# Import Library for manage model part Model Object\n",
    "from keras.models import Model\n",
    "# Import Library for manage model part optimizer\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "# Import Library about model \n",
    "from keras.utils import plot_model\n",
    "# Import library for load model\n",
    "from keras.models import load_model\n",
    "# Import library operation in Keras tensor object\n",
    "from keras import backend as K\n",
    "\n",
    "#Import library for normal process\n",
    "import numpy as np\n",
    "\n",
    "#Import library for manage clear output\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant data path to collect directory data\n",
    "PATH_DATA = \"/home/zeabus/Documents/supasan/2019_deep_learning/AnimeFaceData\" \n",
    "\n",
    "# Part parameter in normal program will require to input\n",
    "crop = True\n",
    "color = True\n",
    "rounds = 10 # Round to train data (epoches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f6aa9859a10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "directory_handle = DirectoryHandle( PATH_DATA )\n",
    "list_file = directory_handle.get_all_file()\n",
    "\n",
    "width , height = ImageHandle.read_size( list_file )\n",
    "width = np.array( width )\n",
    "height = np.array( height )\n",
    "plt.scatter( width , height )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will manage by resize image to size 26 square and have latent size 64\n"
     ]
    }
   ],
   "source": [
    "square_size = int( np.ceil( width.min() ) )\n",
    "square_size = square_size if square_size % 2 == 0 else square_size + 1\n",
    "#latent_size = int( np.power( square_size , 2 ) / 4 * 3 )\n",
    "latent_size = 64\n",
    "print(f'Will manage by resize image to size {square_size} square and have latent size {latent_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = ImageHandle.read_all_data( list_file , square_size , color = color , crop = crop )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "picture_shape = X_data[0].shape\n",
    "shape_before_flatten = ( int( picture_shape[0] / 2),\n",
    "                int( picture_shape[1] / 2),\n",
    "                16 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Model\n",
    "This part I split step to setup GAN Model to Generator, Descriminator and GAN model\n",
    "### Part Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "generator_input (InputLayer) (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "generator_neuron1 (Dense)    (None, 2704)              175760    \n",
      "_________________________________________________________________\n",
      "generator_neuron1_activation (None, 2704)              0         \n",
      "_________________________________________________________________\n",
      "generator_reshape (Reshape)  (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "generator_convolution1 (Conv (None, 13, 13, 16)        2320      \n",
      "_________________________________________________________________\n",
      "generator_convolution1_activ (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "generator_convolution2 (Conv (None, 26, 26, 32)        4640      \n",
      "_________________________________________________________________\n",
      "generator_convolution2_activ (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "generator_output (Conv2DTran (None, 26, 26, 3)         867       \n",
      "_________________________________________________________________\n",
      "generator_output_activation  (None, 26, 26, 3)         0         \n",
      "=================================================================\n",
      "Total params: 183,587\n",
      "Trainable params: 183,587\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator_input = Input( shape=(latent_size,),\n",
    "                name = \"generator_input\")\n",
    "generator = Dense( np.prod( shape_before_flatten ),\n",
    "                name = \"generator_neuron1\" )( generator_input )\n",
    "generator = Activation( \"relu\",\n",
    "                name = \"generator_neuron1_activation\" )(generator)\n",
    "generator = Reshape( shape_before_flatten , \n",
    "                name = \"generator_reshape\" )(generator)\n",
    "generator = Conv2DTranspose( filters = 16,\n",
    "                           kernel_size = (3,3),\n",
    "                           strides = 1,\n",
    "                           padding = \"same\",\n",
    "                           name = \"generator_convolution1\",\n",
    "                           use_bias = True )(generator)\n",
    "generator = Activation( \"relu\",\n",
    "                name = \"generator_convolution1_activation\" )(generator)\n",
    "generator = Conv2DTranspose( filters = 32,\n",
    "                           kernel_size = (3,3),\n",
    "                           strides = 2,\n",
    "                           padding = \"same\",\n",
    "                           name = \"generator_convolution2\" )(generator)\n",
    "generator = Activation( \"relu\",\n",
    "                name = \"generator_convolution2_activation\" )(generator)\n",
    "generator = Conv2DTranspose( filters = 3,\n",
    "                           kernel_size = (3,3),\n",
    "                           strides = 1,\n",
    "                           padding = \"same\",\n",
    "                           name = \"generator_output\" )(generator)\n",
    "generator_output = Activation( \"relu\",\n",
    "                name = \"generator_output_activation\" )(generator)\n",
    "generator_model = Model( generator_input , generator_output )\n",
    "generator_model.name = \"generator\"\n",
    "generator_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "discirminator_input (InputLa (None, 26, 26, 3)         0         \n",
      "_________________________________________________________________\n",
      "discriminator_convolution1 ( (None, 26, 26, 32)        896       \n",
      "_________________________________________________________________\n",
      "discriminator_convolution1_a (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "discriminator_convolution2 ( (None, 13, 13, 16)        4624      \n",
      "_________________________________________________________________\n",
      "discriminator_convolution2_a (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "discriminator_flatten (Flatt (None, 2704)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2704)              0         \n",
      "_________________________________________________________________\n",
      "discriminator_dense1 (Dense) (None, 64)                173120    \n",
      "_________________________________________________________________\n",
      "discriminator_dense1_activat (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "discriminator_output (Dense) (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "discriminator_output_activat (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 178,705\n",
      "Trainable params: 178,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator_input = Input( picture_shape ,\n",
    "                    name = \"discirminator_input\" )\n",
    "discriminator = Conv2D( filters = 32,\n",
    "                    kernel_size = (3,3),\n",
    "                    strides = 1,\n",
    "                    padding = \"same\",\n",
    "                    name = \"discriminator_convolution1\" )( discriminator_input )\n",
    "discriminator = Activation( \"relu\",\n",
    "                name = \"discriminator_convolution1_activation\" )(discriminator)\n",
    "discriminator = Conv2D( filters = 16,\n",
    "                    kernel_size = (3,3),\n",
    "                    strides = 2,\n",
    "                    padding = \"same\",\n",
    "                    name = \"discriminator_convolution2\" )( discriminator )\n",
    "discriminator = Activation( \"relu\",\n",
    "                name = \"discriminator_convolution2_activation\" )(discriminator)\n",
    "discriminator = Flatten( name = \"discriminator_flatten\" )( discriminator )\n",
    "discriminator = Dropout( 0.2 )( discriminator )\n",
    "discriminator = Dense( latent_size ,\n",
    "                name = \"discriminator_dense1\" )( discriminator )\n",
    "discriminator = Activation( \"sigmoid\",\n",
    "                name = \"discriminator_dense1_activation\" )( discriminator )\n",
    "discriminator = Dense( 1 , name = \"discriminator_output\" )( discriminator )\n",
    "discriminator_output = Activation( \"sigmoid\",\n",
    "                name = \"discriminator_output_activation\" )( discriminator )\n",
    "discriminator_model = Model( discriminator_input , discriminator_output )\n",
    "discriminator_model.name = \"discriminator\"\n",
    "discriminator_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"GAN_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "generator_input (InputLayer) (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "generator (Model)            (None, 26, 26, 3)         183587    \n",
      "_________________________________________________________________\n",
      "discriminator (Model)        (None, 1)                 178705    \n",
      "=================================================================\n",
      "Total params: 362,292\n",
      "Trainable params: 362,292\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "GAN_model = Model( generator_input , discriminator_model( generator_model( generator_input ) ) )\n",
    "GAN_model.name = \"GAN_model\"\n",
    "GAN_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter About Training\n",
    "enable_report = True\n",
    "round_train = 2\n",
    "batch_check_point = 10\n",
    "epochs_discriminator = 1\n",
    "epochs_GAN = 2\n",
    "verbose = 1 # 0 is silencs 1 is progress bar 2 is result?\n",
    "batch_size = 2048\n",
    "round_batch = int( np.ceil( len(X_data) / batch_size) )\n",
    "list_latent_vector = []\n",
    "size_latent_vector = (  batch_size , latent_size )\n",
    "real_image = np.array( X_data ).astype( np.float ) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAN_optimizer = RMSprop( lr = 0.0005 )\n",
    "#GAN_optimizer = Adam( lr = 0.0005 )\n",
    "GAN_model.compile( optimizer = GAN_optimizer,\n",
    "                     loss = \"binary_crossentropy\", \n",
    "                     metrics=[ 'accuracy' ] )\n",
    "discriminator_optimizer = RMSprop( lr = 0.0005 )\n",
    "#discriminator_optimizer = Adam( lr = 0.0005 )\n",
    "discriminator_model.compile( optimizer = discriminator_optimizer,\n",
    "                     loss = \"binary_crossentropy\", \n",
    "                     metrics=[ 'accuracy' ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAN_model.load_weights( \"temporary_checkpoint_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 20/32 round 1/2\n",
      "=disciminator : loss 0.6936211896128952 and accuracy 0.492919921875\n",
      "==========gan : loss 0.6964110851287841 and accuracy 0.44999998807907104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> discriminator\n",
      "Epoch 1/1\n",
      "4096/4096 [==============================] - 14s 3ms/step - loss: 0.6933 - accuracy: 0.5068\n",
      "===> GAN\n",
      "Epoch 1/2\n",
      "2048/2048 [==============================] - 15s 8ms/step - loss: 0.6818 - accuracy: 1.0000\n",
      "Epoch 2/2\n",
      "2048/2048 [==============================] - 13s 6ms/step - loss: 0.6818 - accuracy: 1.0000\n",
      "===> discriminator\n",
      "Epoch 1/1\n",
      "4096/4096 [==============================] - 14s 3ms/step - loss: 0.6934 - accuracy: 0.4893\n",
      "===> GAN\n",
      "Epoch 1/2\n",
      "2048/2048 [==============================] - 16s 8ms/step - loss: 0.6958 - accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "2048/2048 [==============================] - 11s 6ms/step - loss: 0.6958 - accuracy: 0.0000e+00\n",
      "===> discriminator\n",
      "Epoch 1/1\n",
      "4096/4096 [==============================] - 13s 3ms/step - loss: 0.6933 - accuracy: 0.4980\n",
      "===> GAN\n",
      "Epoch 1/2\n",
      "2048/2048 [==============================] - 18s 9ms/step - loss: 0.7043 - accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "2048/2048 [==============================] - 13s 6ms/step - loss: 0.7043 - accuracy: 0.0000e+00\n",
      "===> discriminator\n",
      "Epoch 1/1\n",
      "4096/4096 [==============================] - 15s 4ms/step - loss: 0.6934 - accuracy: 0.4819\n",
      "===> GAN\n",
      "Epoch 1/2\n",
      "2048/2048 [==============================] - 14s 7ms/step - loss: 0.7039 - accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "2048/2048 [==============================] - 14s 7ms/step - loss: 0.7039 - accuracy: 0.0000e+00\n",
      "===> discriminator\n",
      "Epoch 1/1\n",
      "4096/4096 [==============================] - 13s 3ms/step - loss: 0.6933 - accuracy: 0.4990\n",
      "===> GAN\n",
      "Epoch 1/2\n",
      "2048/2048 [==============================] - 7s 3ms/step - loss: 0.7097 - accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "2048/2048 [==============================] - 5s 3ms/step - loss: 0.7097 - accuracy: 0.0000e+00\n",
      "===> discriminator\n",
      "Epoch 1/1\n",
      "3136/4096 [=====================>........] - ETA: 2s - loss: 0.6933 - accuracy: 0.4990"
     ]
    }
   ],
   "source": [
    "discriminator_history = { 'loss' : [] , 'accuracy' : [] }\n",
    "gan_history = { 'loss' : [] , 'accuracy' : [] }\n",
    "for count_round in range( round_train ):\n",
    "    discriminator_loss = []\n",
    "    discriminator_accuracy = []\n",
    "    gan_loss = []\n",
    "    gan_accuracy = []\n",
    "\n",
    "    start = 0\n",
    "    count_latent = 0\n",
    "#    all_latent_vector = np.random.normal( 0 , 1 , size = ( round_batch , size_latent_vector[0] , size_latent_vector[1]) )\n",
    "    while( start < real_image.shape[0] ):\n",
    "#        latent_vector = np.random.randint( 0 , 256 , size = size_latent_vector ).astype( np.float ) / 255\n",
    "        latent_vector = np.random.normal( 0 , 1 , size= size_latent_vector )\n",
    "#        latent_vector = all_latent_vector[ count_latent ]\n",
    "        fake_image = generator_model.predict( latent_vector )\n",
    "        if count_latent % batch_check_point == 0 :\n",
    "            fig, axes = plt.subplots(2, 5)\n",
    "            temp = 0\n",
    "            for i in range(2):\n",
    "                for j in range(5):\n",
    "                    axes[i, j].imshow(fake_image[temp] ) \n",
    "                    axes[i, j].axis('off')\n",
    "                    temp += 1\n",
    "            plt.show()\n",
    "        \n",
    "#        print( f'Data batch index {count_latent+1}/{round_batch} on round {count_round+1}/{round_train}')\n",
    "        stop = start + batch_size\n",
    "        if stop > real_image.shape[0] : stop = real_image.shape[0]\n",
    "\n",
    "        all_image = np.concatenate( [ real_image[ start:stop ],\n",
    "                    fake_image ] )\n",
    "        label_image = np.concatenate( [\n",
    "                np.ones( ( stop - start , 1 ) ),\n",
    "                np.zeros( ( size_latent_vector[0] , 1 ) )\n",
    "            ] )\n",
    "#        label_image += 0.05 * np.random.random( label_image.shape )    \n",
    "        # Train Discriminator\n",
    "        if enable_report : print( \"===> discriminator\" , end = \"\" if verbose == 0 else \"\\n\" )\n",
    "#        d_loss , d_accuracy = discriminator_model.train_on_batch( all_image , label_image )\n",
    "        d_history = discriminator_model.fit( all_image , label_image, epochs = epochs_discriminator, verbose = verbose )\n",
    "        if verbose == 0 and enable_report:\n",
    "            print( f' Loss {np.mean(d_history.history[\"loss\"]):10.6f} Accuracy {np.mean(d_history.history[\"accuracy\"]):10.6f}')\n",
    "\n",
    "            \n",
    "#======= Train Generator\n",
    "        \n",
    "        ## Before train must to disable train discriminator\n",
    "        discriminator_model.trainable = False\n",
    "        GAN_model.compile( optimizer = GAN_optimizer,\n",
    "                        loss = \"binary_crossentropy\", \n",
    "                        metrics=[ 'accuracy' ] )\n",
    "        discriminator_model.compile( optimizer = discriminator_optimizer,\n",
    "                        loss = \"binary_crossentropy\", \n",
    "                        metrics=[ 'accuracy' ] )\n",
    "        ## End part disable train discriminator\n",
    "        \n",
    "        if enable_report : print( \"===> GAN\" , end = \"\" if verbose == 0 else \"\\n\" )        \n",
    "#        latent_vector = np.random.randint( 0 , 256 , size = size_latent_vector ).astype( np.float ) / 255\n",
    "        latent_vector = np.random.normal( 0 , 1 , size= size_latent_vector )\n",
    "        g_history = GAN_model.fit( latent_vector, np.ones( ( size_latent_vector[0] , 1 ) ),\n",
    "                                  epochs = epochs_GAN ,\n",
    "                                  verbose = verbose )\n",
    "#        g_loss , g_accuracy = GAN_model.train_on_batch( latent_vector, np.ones( ( size_latent_vector[0] , 1 ) ) )\n",
    "        if verbose == 0 and enable_report :\n",
    "            print( f' Loss {np.mean(g_history.history[\"loss\"]):10.6f} Accuracy {np.mean(g_history.history[\"accuracy\"]):10.6f}')\n",
    "            \n",
    "        ## After train must to enable train discriminator\n",
    "        discriminator_model.trainable = True\n",
    "        GAN_model.compile( optimizer = GAN_optimizer,\n",
    "                        loss = \"binary_crossentropy\", \n",
    "                        metrics=[ 'accuracy' ] )\n",
    "        discriminator_model.compile( optimizer = discriminator_optimizer,\n",
    "                        loss = \"binary_crossentropy\", \n",
    "                        metrics=[ 'accuracy' ] )\n",
    "\n",
    "#======== Part add data to report\n",
    "#        discriminator_loss.append( d_loss )\n",
    "#        discriminator_accuracy.append( d_accuracy )\n",
    "#        gan_loss.append( g_loss )\n",
    "#        gan_accuracy.append( g_accuracy )\n",
    "        discriminator_loss += d_history.history[ 'loss']\n",
    "        discriminator_accuracy += d_history.history[ 'accuracy' ]\n",
    "        gan_loss += g_history.history[ 'loss' ]\n",
    "        gan_accuracy += g_history.history[ 'accuracy' ]\n",
    "        start = stop        \n",
    "        count_latent += 1\n",
    "        \n",
    "        if count_latent % batch_check_point == 0 :\n",
    "            clear_output()\n",
    "            print( f'At {count_latent}/{round_batch} round {count_round+1}/{round_train}')\n",
    "            print( f'=disciminator : loss {np.mean( discriminator_loss ) } and accuracy {np.mean( discriminator_accuracy ) }' )\n",
    "            print( f'==========gan : loss {np.mean(gan_loss)} and accuracy {np.mean(gan_accuracy) }')        \n",
    "        \n",
    "        if count_latent % (batch_check_point * 2 ) == 0:\n",
    "            GAN_model.save_weights( \"temporary_checkpoint_weights.h5\")\n",
    "            GAN_model.save( \"temporary_checkpoint_model.h5\")   \n",
    "         \n",
    "    clear_output()\n",
    "    print( f'Summary round {count_round+1}/{round_train}')  \n",
    "    print( f'=disciminator : loss {np.mean( discriminator_loss ) } and accuracy {np.mean( discriminator_accuracy ) }' )\n",
    "    print( f'==========gan : loss {np.mean(gan_loss)} and accuracy {np.mean(gan_accuracy) }')\n",
    "    GAN_model.save_weights( \"temporary_checkpoint_weights.h5\")\n",
    "    GAN_model.save( \"temporary_checkpoint_model.h5\")\n",
    "    \n",
    "    discriminator_history['loss'].append( discriminator_loss )\n",
    "    discriminator_history['accuracy'].append( discriminator_accuracy )\n",
    "    gan_history['loss'].append( gan_loss )\n",
    "    gan_history['accuracy'].append( gan_accuracy )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAN_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_random = np.random.normal( 0 , 1 , size=( 10 , latent_size ))\n",
    "#latent_random = np.random.randint( 0 , 256 , size = ( 10 , latent_size ) ).astype( np.float ) / 255\n",
    "DataHandle.plot( latent_random , generator_model , dest_type = float )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
